# Fundamentals
This chapter will describe how to get started with the Web Audio API, which brows‐ ers are supported, how to detect if the API is available, what an audio graph is, what audio nodes are, how to connect nodes together, some basic node types, and finally, how to load sound files and playback sounds.


# 基础

此章将描述从何处入手 Web Audio API, 哪些浏览器支持，如何检测哪些 API 可用，什么是音频可视化，音频节点是什么，如何将音频节点组合连接在一起，介绍一些基础的音频节点类型，最后加载音频并播放


## A Brief History of Audio on the Web
The first way of playing back sounds on the web was via the <bgsound> tag, which let website authors automatically play background music when a visitor opened their pages. This feature was only available in Internet Explorer, and was never standar‐ dized or picked up by other browsers. Netscape implemented a similar feature with the <embed> tag, providing basically equivalent functionality.
Flash was the first cross-browser way of playing back audio on the Web, but it had the significant drawback of requiring a plug-in to run. More recently, browser vendors have rallied around the HTML5 <audio> element, which provides native support for audio playback in all modern browsers.
Although audio on the Web no longer requires a plug-in, the <audio> tag has signifi‐ cant limitations for implementing sophisticated games and interactive applications. The following are just some of the limitations of the <audio> element:

• No precise timing controls
• Very low limit for the number of sounds played at once
• No way to reliably pre-buffer a sound
• No ability to apply real-time effects
• No way to analyze sounds

## Audio 音频在 Web中的历史简介

最早在网页中播放音频使用的是 <bgsound> 标签，当有人访问网页时，网页创作者使用此标签就可自动播放背景音频。这个特性只能 IE 浏览器上可用，它从未被标准化或被其它浏览器所实现过。网景浏览器实现了一个类似的 <embed> 标签, 提供了基本类似的播放功能。

Flash 最早在网页中实现了跨浏览器播放音频的功能，但它有个比较大的缺点就是需要以插件方式运行。最近浏览器开发商都围绕在 HTML5 <audio> 标签，在现代浏览器上此标签提供了原生播放音频的能力。

尽管在网页上播放音频不再需要插件了，但 <audio> 标签在实现一些复杂的游戏和交互性应用时还是有些显著的限制，一些限制如下：

* 没有精确可控时间
* 一次可播放的音频数量太少
* 没能可靠的预缓冲能力
* 无法应用实时音频特效
* 没有可用的方法对音频进行分析


There have been several attempts to create a powerful audio API on the Web to address some of the limitations I previously described. One notable example is the Audio Data API that was designed and prototyped in Mozilla Firefox. Mozilla’s approach started with an <audio> element and extended its JavaScript API with addi‐ tional features. This API has a limited audio graph (more on this later in “The Audio Context” on page 3), and hasn’t been adopted beyond its first implementation. It is currently deprecated in Firefox in favor of the Web Audio API.

他们多次尝试推出强大的 audio API 来解决我以上面提到的音频限制。其中值得注意的就是火狐浏览器设计的 Audio Data API 原型实现。Mozilla 试图用 <audio> 标签对其 Javascript API 进行扩展额外的特性。此 API 在音频的可视化上有限制(更多信息我将在第3章的" The Audio Context" 介绍)，它并未在后续超越 <audio> 的实现。此 API 现已被火狐浏览器放弃


In contrast with the Audio Data API, the Web Audio API is a brand new model, com‐ pletely separate from the <audio> tag, although there are integration points with other web APIs (see Chapter 7). It is a high-level JavaScript API for processing and synthesizing audio in web applications. The goal of this API is to include capabilities found in modern game engines and some of the mixing, processing, and filtering tasks that are found in modern desktop audio production applications. The result is a versatile API that can be used in a variety of audio-related tasks, from games, to inter‐ active applications, to very advanced music synthesis applications and visualizations.

相比于 Audio Data API,  Web Audio API 使用了全新的模式，完全独立于 <audio> 标签的实现，尽管有些点与其它 web API 有混合（详见第7章）。它在 web 应用中提供了更高级的 Javascript API 用于处理和混合音频。此 API 的目的是实现现代游戏所需的能力，包括混音、处理、滤镜特效等这些在现代桌面应用程序产品所能提供的能力。最终结果是一个多功能的 API 可以被用于多样的处理音频相关的任务，从游戏到可交互应用，到高级音频合成应用与可视化。


## Games and Interactivity
Audio is a huge part of what makes interactive experiences so compelling. If you don’t believe me, try watching a movie with the volume muted.
Games are no exception! My fondest video game memories are of the music and sound effects. Now, nearly two decades after the release of some of my favorites, I still can’t get Koji Kondo’s Zelda and Matt Uelmen’s Diablo soundtracks out of my head. Even the sound effects from these masterfully-designed games are instantly recogniz‐ able, from the unit click responses in Blizzard’s Warcraft and Starcraft series to sam‐ ples from Nintendo’s classics.


## 游戏与交互

音频是交互体验的重要组成部分。如果你不信，试着观看一部关掉音频的电影。

在游戏中更是如此！我最爱的电子游戏令我记忆最深的就是其中的音乐与音效。就算过了将近20年，塞尔达和暗黑破坏神的音效在我的脑海中仍然挥之不去。
甚至从暴雪的《魔兽争霸》和《星际争霸》系列中的圈点士兵音效到任天堂经典游戏中的各种音效，这些精心设计的游戏音效立马就可以被识别出来。


Sound effects matter a great deal outside of games, too. They have been around in user interfaces (UIs) since the days of the command line, where certain kinds of errors would result in an audible beep. The same idea continues through modern UIs, where well-placed sounds are critical for notifications, chimes, and of course audio and video communication applications like Skype. Assistant software such as Google Now and Siri provide rich, audio-based feedback. As we delve further into a world of ubiquitous computing, speech- and gesture-based interfaces that lend themselves to screen-free interactions are increasingly reliant on audio feedback. Finally, for visu‐ ally impaired computer users, audio cues, speech synthesis, and speech recognition are critically important to create a usable experience.

音效在游戏外的应用也同样重要。它们在命令行工具内交互之始就被应用于 UI 交互，当输出出错时就发出“哔”的一声。同样它被应用现代交互 UI 上，一般用于提醒功能，铃声，当然也应用于音视频通讯比如 Skype。像 Google Now 和 Siri 这样的助手软件同样提供了丰富的音效反馈。当我们深入发掘这个世界，通用计算设备，语音和手势交互等可脱离屏幕的交互方式更加的依赖于音频的反馈。最后，对于视障视弱的计算机用户来说，音频提示，语音合成与识别提供了最主要的用户体验原则


Interactive audio presents some interesting challenges. To create convincing in-game music, designers need to adjust to all the potentially unpredictable game states a player can find herself in. In practice, sections of the game can go on for an unknown duration, and sounds can interact with the environment and mix in complex ways, requiring environment-specific effects and relative sound positioning. Finally, there can be a large number of sounds playing at once, all of which need to sound good together and render without introducing quality and performance penalties.

可交互的音频也代表着一些有趣的挑战。为了创建合适的游戏音频，设计师需要调整好游戏玩家所有不可预知的状态。在实践中，游戏中某部分的时长可能是未知的，音效与周边环境交互产生更复杂的混音，需要环境特效音且取决于相关的音频位置。最终可能同一时刻播放着一堆音频，全都得保证组合在一起的音效即保证质量又不影响渲染性能




## The Audio Context
The Web Audio API is built around the concept of an audio context. The audio con‐ text is a directed graph of audio nodes that defines how the audio stream flows from its source (often an audio file) to its destination (often your speakers). As audio passes through each node, its properties can be modified or inspected. The simplest audio context is a connection directly form a source node to a destination node

An audio context can be complex, containing many nodes between the source and destination (Figure 1-2) to perform arbitrarily advanced synthesis or analysis.

Figures 1-1 and 1-2 show audio nodes as blocks. The arrows represent connections between nodes. Nodes can often have multiple incoming and outgoing connections. By default, if there are multiple incoming connections into a node, the Web Audio API simply blends the incoming audio signals together.

The concept of an audio node graph is not new, and derives from popular audio frameworks such as Apple’s CoreAudio, which has an analogous Audio Processing Graph API. The idea itself is even older, originating in the 1960s with early audio environments like Moog modular synthesizer systems.


## Audio Context

Web Audio API 是建立在 audio context 音频上下文的概念之上的。音频上下文是描述音频所有节点的，这些节点定义了音频流是如何从起点(一般是音频文件)到目的地(一般就是你的扬声器)。当音频通过每个节点时，音频的属性可被修改也可被查验。最简单的音频上下文就是起始节点到结束节点的直连如图 1-1

![images](./ch-01/img/1-1.png)

图 1-1

一个音频上下文可能会非常复杂，在音频的起点与结束节点之间包含众多的音频节点（图1-2）进行任意众多的高级合成或分析。

图 1-1 和 1-2 方块表示音频节点。箭头代表节点之间的连接。这些节点经常拥有众多的输入和输出连接。默认情况下，如果一个音频节点拥有多个输入源，Web Audio API 会简单的混成一个音频信号。

音频节点图的概念并不新鲜，它来源于一些流行的音频处理框架，如 Apple 的 CoreAudio, 它提供了类似的音频处理 图像 API。它本身的概念很久远了，可追溯到1960年代的早期处理音频，比如 Moog 模块化混成系统（Moog modular synthesizer systems）。

![images](./ch-01/img/1-1.png)

图 1-2






## Initializing an Audio Context
The Web Audio API is currently implemented by the Chrome and Safari browsers (including MobileSafari as of iOS 6) and is available for web developers via JavaScript. In these browsers, the audio context constructor is webkit-prefixed, meaning that instead of creating a new AudioContext, you create a new webkitAudioContext. However, this will surely change in the future as the API stabilizes enough to ship un- prefixed and as other browser vendors implement it. Mozilla has publicly stated that they are implementing the Web Audio API in Firefox, and Opera has started partici‐ pating in the working group.
With this in mind, here is a liberal way of initializing your audio context that would include other implementations (once they exist):


A single audio context can support multiple sound inputs and complex audio graphs, so generally speaking, we will only need one for each audio application we create. The audio context instance includes many methods for creating audio nodes and manipu‐ lating global audio preferences. Luckily, these methods are not webkit-prefixed and are relatively stable. The API is still changing, though, so be aware of breaking changes (see Appendix A).

## 初始化音频上下文

Web Audio API 现已被 Chrome 和 Safari 浏览器实现（包含 IOS 6 手机版 Safari ）并且通过 JavaScript 开放给了 网页开发者。在这些浏览器上，音频上下文创建时需要加上 webkit 前缀，你不能直接使用 `new AudioContext` 而应该使用 `new webkitAudioContext` 创建。然而在未来在 API 足够稳定且多数浏览器供应商都实现了后前缀可去掉。Mozilla 已宣布在火狐浏览器上实现 Web Audio API，Opera 也开始参与进工作组。记得这一点即可，下面是通用的写法可兼容后期（一旦其它浏览器供应商也实现）：

```
var contextClass = (window.AudioContext || window.webkitAudioContext || window.mozAudioContext || window.oAudioContext || window.msAudioContext);
if (contextClass) {
  // Web Audio API is available. var context = new contextClass();
} else {
  // Web Audio API is not available. Ask the user to use a supported browser.
}
```

一个音频上下文即可支持多个音频的输入与复杂的音频图谱，一般来讲，一个应用使用一个音频上下文即可。音频上班文件的实例包含了众多的方法用于创建音频节点以及操作全局音频属性。幸运的是这些方法不需要加前缀，且相对稳定。 API 仍在变更，所以还是要小心会有大的变化。




## Types of Web Audio Nodes
One of the main uses of audio contexts is to create new audio nodes. Broadly speak‐ ing, there are several kinds of audio nodes:
Source nodes
Sound sources such as audio buffers, live audio inputs, <audio> tags, oscillators, and JS processors
Modification nodes
Filters, convolvers, panners, JS processors, etc.
Analysis nodes
Analyzers and JS processors
Destination nodes
Audio outputs and offline processing buffers

Sources need not be based on sound files, but can instead be real-time input from a live instrument or microphone, redirection of the audio output from an audio ele‐ ment [see “Setting Up Background Music with the <audio> Tag” on page 53], or entirely synthesized sound [see “Audio Processing with JavaScript” on page 51]. Though the final destination-node is often the speakers, you can also process without sound playback (for example, if you want to do pure visualization) or do offline pro‐ cessing, which results in the audio stream being written to a destination buffer for later use.




## 音频节点的类型

音频上下文主要的一个功能就是创建新的音频节点。广义上来讲，一般包含以下几个节点类型：

* 源节点（Source nodes）
  音源，如音频缓冲，直播音频输入，<audio> 标签， 振荡器，以及 JS 处理器

* 修饰节点（Modification nodes）
  Filters, convolvers, panners, JS 处理器 等等

* 分析节点（Analysis nodes）
  分析器， JS 处理器
  
* 结束节点（Destination nodes）
  音频输出和结束处理缓存

音频源不限于音频文件，可用直播音频流或麦克风，<audio> 标签内的输出也可以 ，或者整个都是合成音频。尽管最终结束节点一般都是扬声器，不播放音频的情况下你也可以处理(例如，你只希望直接对音频进行可视化)或离线处理，成音频流保存起来用于后续使用。



## Connecting the Audio Graph
Any audio node’s output can be connected to any other audio node’s input by using the connect() function. In the following example, we connect a source node’s output into a gain node, and connect the gain node’s output into the context’s destination:


Note that context.destination is a special node that is associated with the default audio output of your system. The resulting audio graph of the previous code looks like Figure 1-3.

Once we have connected up a graph like this we can dynamically change it. We can disconnect audio nodes from the graph by calling node.disconnect(outputNumber). For example, to reroute a direct connection between source and destination, circum‐ venting the intermediate node, we can do the following:

## 连接音频图

使用 connect() 方法可以将任意音频节点输出连接至任意其它的音频节点。在以下例子，我们连接了音源节点的输出到 gain 节点，并将其输出连接到了结束节点：

```
// Create the source.
var source = context.createBufferSource(); // Create the gain node.
var gain = context.createGain();
// Connect source to filter, filter to destination.
source.connect(gain);
gain.connect(context.destination);
```

注意，`context.destination` 是一个特殊的节点，一般为系统的默认音频输出。上述代码形成的音频图如下 1-3：

![images](./ch-01/img/1-3.png)

图 1-3

一旦像上面这样我们连接了音频图就可以动态改变它。我们可以调用 `node.disconnect(outputNumber)` 来断开节点连接。例如，重新直连源音频节点至结束音频节点，避开中间 gain 节点 代码如下：

```
source.disconnect(0);
gain.disconnect(0);
source.connect(context.destination);
```








